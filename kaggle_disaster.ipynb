{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O9_MotqPry-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "from collections import Counter\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "sns.set_theme()\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,Dataset\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqyCm3sYV70Q",
        "outputId": "371dc3bc-5b40-45fb-9536-049a9365d218"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "STOPWORDS = stopwords.words(\"english\")\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIfLWbLTV-iq"
      },
      "outputs": [],
      "source": [
        "def clean_text(text, lower=True, stem=False, stopwords=STOPWORDS):\n",
        "    # Remove stopwords\n",
        "    if len(stopwords):\n",
        "        pattern = re.compile(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
        "        text = pattern.sub('', text)\n",
        "\n",
        "    # Spacing and filters\n",
        "    text = re.sub(\n",
        "        r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text\n",
        "    )  # add spacing between objects to be filtered\n",
        "    clean=text\n",
        "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n",
        "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
        "    text = text.strip()  # strip white space at the ends\n",
        "    text=re.sub(\"pic.twitter\\S+\",\"\",text)\n",
        "    text=re.sub(\"@\\S+\",\"\",text)\n",
        "    text = re.sub('#', '', text)\n",
        "    text = re.sub('goooooooaaaaaal', 'goal', text)\n",
        "    text = re.sub('SOOOO', 'SO', text)\n",
        "    text = re.sub('LOOOOOOL', 'LOL', text)\n",
        "    text = re.sub('Cooool', 'cool', text)\n",
        "    text = re.sub('|', '', text)\n",
        "    text = re.sub(r'\\?{2,}', '? ', text)\n",
        "    text = re.sub(r'\\.{2,}', '. ', text)\n",
        "    text = re.sub(r'\\!{2,}', '! ', text)\n",
        "    text = re.sub('&amp;', '&', text)\n",
        "    text = re.sub('Comin', 'Coming', text)\n",
        "    text = re.sub('&gt;', '> ', text)\n",
        "    text = re.sub('&lt;', '< ', text)\n",
        "    text = re.sub(r'.:', '', text)\n",
        "    text = re.sub('baaaack', 'back', text)\n",
        "    text = re.sub('RT', '', text)\n",
        "    text = re.sub('\\s{2,}', ' ', text)\n",
        "    # Remove links\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "\n",
        "    # Stemming\n",
        "    if stem:\n",
        "        text = \" \".join([stemmer.stem(word, to_lowercase=lower) for word in text.split(\" \")])\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0FmIuddZQ9Fc",
        "outputId": "c8d62ffc-d9b6-4937-e61a-1e5ff9e2aae6"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1fdMuD9VvkY",
        "outputId": "82330fcc-cfdb-4302-9f8c-2b6fa5d7df9e"
      },
      "outputs": [],
      "source": [
        "original_df = train.copy()\n",
        "train.text =train.text.apply(clean_text, lower=True, stem=False)\n",
        "print (f\"{original_df.text.values[0]}\\n{train.text.values[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_variable = 'target'\n",
        "train_df, test_df = train_test_split(train, test_size=0.2, random_state=42, stratify=train[target_variable])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def append_keyword(df):\n",
        "    for index, row in df.iterrows():\n",
        "        if not pd.isna(row['keyword']):\n",
        "            df.at[index, 'text'] = row['keyword'] + ' ' + row['text']\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df=append_keyword(train_df)\n",
        "test_df=append_keyword(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cQlOAtxYKMA",
        "outputId": "29085ed4-c3f0-46cc-fc3b-6d6989e30b1b"
      },
      "outputs": [],
      "source": [
        "# Define a custom dataset class\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, targets, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        target = self.targets[item]\n",
        "        \n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            return_token_type_ids=False,\n",
        "            padding=\"max_length\",\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# Split the data into input and target columns\n",
        "train_texts = train_df[\"text\"].tolist()\n",
        "train_labels = train_df[\"target\"].tolist()\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Initialize the dataset and data loader\n",
        "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
        "\n",
        "# Define the optimizer and learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.train()\n",
        "epochs=3\n",
        "for epoch in range(epochs):\n",
        "    print(\"gah\")\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n",
        "        loss = outputs.loss\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"my_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_label(text):\n",
        "    encoding = tokenizer.encode_plus(text, add_special_tokens=True, max_length=128,\n",
        "                                     padding='max_length', return_attention_mask=True,\n",
        "                                     return_token_type_ids=False, return_tensors='pt')\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    predictions = torch.argmax(outputs[0], dim=1)\n",
        "    return predictions.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test['target'] = test['text'].apply(predict_label)\n",
        "result_df = test[['id', 'target']]\n",
        "result_df.to_csv('output.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = pd.read_csv(test_path)\n",
        "original_df = test.copy()\n",
        "test.text =test.text.apply(clean_text, lower=True, stem=False)\n",
        "print (f\"{original_df.text.values[0]}\\n{test.text.values[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_accuracy(df):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for index, row in df.iterrows():\n",
        "        predicted_label = predict_label(row['text'])\n",
        "        actual_label = row['target']\n",
        "        if predicted_label == actual_label:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
